{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af4ffc72",
      "metadata": {
        "id": "af4ffc72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56ffe8c7-39e9-4ea1-dc9b-66bfa45b81b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.7 MB 33.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 57.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 120 kB 55.7 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  from transformers import BertTokenizerFast, BertConfig, BertForMaskedLM\n",
        "except:\n",
        "  !pip -q install transformers\n",
        "  from transformers import BertTokenizerFast, BertConfig, BertForMaskedLM, AdamW, get_scheduler\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install datasets\n",
        "from datasets import Dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEN1C9V8gH0r",
        "outputId": "6f3c6dd4-5782-43aa-c08a-d7937b7a01c3"
      },
      "id": "gEN1C9V8gH0r",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 365 kB 24.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 212 kB 20.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 115 kB 60.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 127 kB 61.2 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install optuna\n",
        "import optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiJQ4vO-b370",
        "outputId": "5576f4c0-6234-4d05-d9a1-95b3839ce136"
      },
      "id": "yiJQ4vO-b370",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 348 kB 31.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 209 kB 70.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 81 kB 11.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 78 kB 8.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 112 kB 27.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 49 kB 6.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 147 kB 75.4 MB/s \n",
            "\u001b[?25h  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "493cbbc6",
      "metadata": {
        "id": "493cbbc6"
      },
      "outputs": [],
      "source": [
        "tokens = [\"[MASK]\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\"]\n",
        "\n",
        "## Helper Functions\n",
        "\n",
        "def sudoku_to_tokens(sudoku_str):\n",
        "  return [tokens[int(i)] for i in sudoku_str]\n",
        "\n",
        "def tokens_to_sudoku(token_list):\n",
        "  return \"\".join(str(tokens.index(x)) for x in token_list)\n",
        "\n",
        "def display_sudoku(sudoku_str):\n",
        "  print(\"-\"*21)\n",
        "  for i in range(0, 9):\n",
        "    for j in range(0, 9):\n",
        "      if sudoku_str[i*9+j] == \"0\": print(\"  \", end=\"\")\n",
        "      else: print(sudoku_str[i*9+j] + \" \", end=\"\")\n",
        "      if j in [2, 5]: print(\"| \", end=\"\")\n",
        "    print(\"\")\n",
        "    if i in [2,5,8]: print(\"-\" * 21)\n",
        "\n",
        "def fill_masks(sudoku_str, mask_str):\n",
        "  ret_str = \"\"\n",
        "  mask = iter(mask_str)\n",
        "  for s in sudoku_str:\n",
        "    if int(s) != 0:\n",
        "      ret_str += s\n",
        "    else:\n",
        "      ret_str += next(mask)\n",
        "  return ret_str\n",
        "\n",
        "def check_correctness(sudoku_str):\n",
        "  horizontals = [[sudoku_str[i+j*9] for i in range(0, 9)] for j in range(0,9)]\n",
        "  verticals = [[sudoku_str[i*9+j] for i in range(0, 9)] for j in range(0,9)]\n",
        "  boxes = [[sudoku_str[i*3+j*3*9 + x+y*9] for x in range(0, 3) for y in range(0,3)]\n",
        "            for i in range(0, 3) for j in range(0,3)]\n",
        "  horizontals_correct = all(map(lambda x: len(set(x)) == 9, horizontals))\n",
        "  verticals_correct = all(map(lambda x: len(set(x)) == 9, verticals))\n",
        "  boxes_correct = all(map(lambda x: len(set(x)) == 9, boxes))\n",
        "  return horizontals_correct and verticals_correct and boxes_correct"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ],
      "metadata": {
        "id": "QQTyVMI1vWjX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90db7bd8-8241-44be-afef-6e7fa63e16b7"
      },
      "id": "QQTyVMI1vWjX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uploading Kaggle Json\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "# Then move kaggle.json into the folder where the API expects to find it.\n",
        "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgZG8gewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwoKICAgICAgbGV0IHBlcmNlbnREb25lID0gZmlsZURhdGEuYnl0ZUxlbmd0aCA9PT0gMCA/CiAgICAgICAgICAxMDAgOgogICAgICAgICAgTWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCk7CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPSBgJHtwZXJjZW50RG9uZX0lIGRvbmVgOwoKICAgIH0gd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCk7CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "K-0PmnDgHa2U",
        "outputId": "38d2de9b-39c2-4794-ff77-890681bdf0c6"
      },
      "id": "K-0PmnDgHa2U",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2ade6e52-368c-4b3e-856a-8588406a8791\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2ade6e52-368c-4b3e-856a-8588406a8791\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "User uploaded file \"kaggle.json\" with length 62 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading sudoku dataset from kaggle\n",
        "!kaggle datasets download -d rohanrao/sudoku\n",
        "!unzip sudoku.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pe1bHU4KIPQI",
        "outputId": "b619b51b-e743-4250-e886-354231c26890"
      },
      "id": "Pe1bHU4KIPQI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading sudoku.zip to /content\n",
            " 98% 611M/620M [00:05<00:00, 156MB/s]\n",
            "100% 620M/620M [00:05<00:00, 127MB/s]\n",
            "Archive:  sudoku.zip\n",
            "  inflating: sudoku.csv              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80f245a7",
      "metadata": {
        "id": "80f245a7"
      },
      "outputs": [],
      "source": [
        "# path =\"/content/drive/MyDrive/sudoku2.csv\"\n",
        "path =\"/content/sudoku.csv\"\n",
        "df = pd.read_csv(path, nrows=300000) # only read x rows\n",
        "data=df.sample(frac=0.8,random_state=200)\n",
        "test=df.drop(data.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc3a0fb9",
      "metadata": {
        "id": "cc3a0fb9"
      },
      "outputs": [],
      "source": [
        "vocab = \"[PAD]\\n[UNK]\\n[CLS]\\n[SEP]\\n[MASK]\\none\\ntwo\\nthree\\nfour\\nfive\\nsix\\nseven\\neight\\nnine\"\n",
        "os.makedirs(\"/content/tokenizer\", exist_ok=True)\n",
        "with open(\"/content/tokenizer/vocab.txt\", \"w\") as f:\n",
        "  f.write(vocab)\n",
        "\n",
        "\n",
        "q = data.iloc[:, 0].values\n",
        "s = data.iloc[:, 1].values\n",
        "x1 = list(map(sudoku_to_tokens, q))\n",
        "y1 = list(map(sudoku_to_tokens, s))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter Tuning using Optuna Framework\n",
        "\n",
        "def objective(trial):\n",
        "\n",
        "  q1 = data.iloc[:50000, 0].values\n",
        "  s1 = data.iloc[:50000, 1].values\n",
        "  x2 = list(map(sudoku_to_tokens, q1))\n",
        "  y2 = list(map(sudoku_to_tokens, s1))\n",
        "  \n",
        "  tokenizer = BertTokenizerFast.from_pretrained('/content/tokenizer')\n",
        "  hidden_size = trial.suggest_int(\"hidden\", 20, 400, step = 20, log=False )\n",
        "  hidden_layers = trial.suggest_int(\"hid_layers\", 2, 12, step = 2, log=False)\n",
        "  config = BertConfig(\n",
        "      vocab_size=14,\n",
        "      max_position_embeddings=83,\n",
        "      hidden_size=hidden_size,\n",
        "      num_attention_heads=10,\n",
        "      num_hidden_layers=hidden_layers,\n",
        "      type_vocab_size=1\n",
        "      )\n",
        "  param_model = BertForMaskedLM(config)\n",
        "  \n",
        "  inputs = tokenizer.batch_encode_plus(x2, return_tensors=\"pt\",is_split_into_words=True)\n",
        "  labels = tokenizer.batch_encode_plus(y2, return_tensors=\"pt\",is_split_into_words=True)\n",
        "  inputs['labels']=labels['input_ids']\n",
        "  \n",
        "  dataset = Dataset.from_dict(inputs)\n",
        "  dataset.set_format(\"torch\")\n",
        "  train_dataloader = DataLoader(dataset, shuffle=True, batch_size=32)\n",
        "  \n",
        "  device = torch.device('cuda:0')\n",
        "  param_model.to(device)\n",
        "\n",
        "  # activate training model\n",
        "  param_model.train()\n",
        "  # initialize optimizer\n",
        "  optim = AdamW(param_model.parameters(), lr=1e-4)\n",
        "\n",
        "  param_model.train()\n",
        "  for batch in train_dataloader:\n",
        "      batch = {k: v.to(device) for k, v in batch.items()}\n",
        "      outputs = param_model(**batch)\n",
        "      loss = outputs.loss\n",
        "      loss.backward()  \n",
        "      optim.step()\n",
        "      optim.zero_grad()\n",
        "  return loss"
      ],
      "metadata": {
        "id": "vR1BtmBSX-F5",
        "cellView": "form"
      },
      "id": "vR1BtmBSX-F5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wu2A25eWbssf",
        "outputId": "68abd760-5dee-4483-ddf7-0825fd372169",
        "cellView": "form"
      },
      "id": "wu2A25eWbssf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-08 11:34:35,150]\u001b[0m A new study created in memory with name: no-name-dd3c318a-f94a-4ab1-9afa-3a3cad2d98a9\u001b[0m\n",
            "\u001b[32m[I 2022-09-08 11:35:44,029]\u001b[0m Trial 0 finished with value: 1.0670064687728882 and parameters: {'hidden': 100, 'hid_layers': 2}. Best is trial 0 with value: 1.0670064687728882.\u001b[0m\n",
            "\u001b[32m[I 2022-09-08 11:37:55,306]\u001b[0m Trial 1 finished with value: 1.0747731924057007 and parameters: {'hidden': 180, 'hid_layers': 6}. Best is trial 0 with value: 1.0670064687728882.\u001b[0m\n",
            "\u001b[32m[I 2022-09-08 11:39:32,040]\u001b[0m Trial 2 finished with value: 0.3555595874786377 and parameters: {'hidden': 340, 'hid_layers': 2}. Best is trial 2 with value: 0.3555595874786377.\u001b[0m\n",
            "\u001b[32m[I 2022-09-08 11:43:58,421]\u001b[0m Trial 3 finished with value: 1.0928794145584106 and parameters: {'hidden': 220, 'hid_layers': 12}. Best is trial 2 with value: 0.3555595874786377.\u001b[0m\n",
            "\u001b[32m[I 2022-09-08 11:46:53,216]\u001b[0m Trial 4 finished with value: 0.3889857828617096 and parameters: {'hidden': 400, 'hid_layers': 4}. Best is trial 2 with value: 0.3555595874786377.\u001b[0m\n",
            "\u001b[32m[I 2022-09-08 11:48:34,040]\u001b[0m Trial 5 finished with value: 1.0684183835983276 and parameters: {'hidden': 180, 'hid_layers': 4}. Best is trial 2 with value: 0.3555595874786377.\u001b[0m\n",
            "\u001b[32m[I 2022-09-08 11:52:35,571]\u001b[0m Trial 6 finished with value: 0.3219452202320099 and parameters: {'hidden': 400, 'hid_layers': 6}. Best is trial 6 with value: 0.3219452202320099.\u001b[0m\n",
            "\u001b[32m[I 2022-09-08 11:57:15,949]\u001b[0m Trial 7 finished with value: 1.1689916849136353 and parameters: {'hidden': 380, 'hid_layers': 8}. Best is trial 6 with value: 0.3219452202320099.\u001b[0m\n",
            "\u001b[32m[I 2022-09-08 11:59:11,364]\u001b[0m Trial 8 finished with value: 0.8770503997802734 and parameters: {'hidden': 240, 'hid_layers': 4}. Best is trial 6 with value: 0.3219452202320099.\u001b[0m\n",
            "\u001b[32m[I 2022-09-08 12:01:30,422]\u001b[0m Trial 9 finished with value: 0.4093311131000519 and parameters: {'hidden': 300, 'hid_layers': 4}. Best is trial 6 with value: 0.3219452202320099.\u001b[0m\n",
            "\u001b[32m[I 2022-09-08 12:03:31,078]\u001b[0m Trial 10 finished with value: 1.3452699184417725 and parameters: {'hidden': 20, 'hid_layers': 10}. Best is trial 6 with value: 0.3219452202320099.\u001b[0m\n",
            "\u001b[32m[I 2022-09-08 12:05:01,917]\u001b[0m Trial 11 finished with value: 0.4791155755519867 and parameters: {'hidden': 320, 'hid_layers': 2}. Best is trial 6 with value: 0.3219452202320099.\u001b[0m\n",
            "\u001b[32m[I 2022-09-08 12:09:24,798]\u001b[0m Trial 12 finished with value: 1.0967128276824951 and parameters: {'hidden': 340, 'hid_layers': 8}. Best is trial 6 with value: 0.3219452202320099.\u001b[0m\n",
            "\u001b[32m[I 2022-09-08 12:12:27,448]\u001b[0m Trial 13 finished with value: 1.0902719497680664 and parameters: {'hidden': 280, 'hid_layers': 6}. Best is trial 6 with value: 0.3219452202320099.\u001b[0m\n",
            "\u001b[32m[I 2022-09-08 12:14:14,747]\u001b[0m Trial 14 finished with value: 0.3461976647377014 and parameters: {'hidden': 400, 'hid_layers': 2}. Best is trial 6 with value: 0.3219452202320099.\u001b[0m\n",
            "\u001b[32m[I 2022-09-08 12:20:30,968]\u001b[0m Trial 15 finished with value: 1.0477865934371948 and parameters: {'hidden': 400, 'hid_layers': 10}. Best is trial 6 with value: 0.3219452202320099.\u001b[0m\n",
            "\u001b[32m[I 2022-09-08 12:24:03,952]\u001b[0m Trial 16 finished with value: 0.5534767508506775 and parameters: {'hidden': 360, 'hid_layers': 6}. Best is trial 6 with value: 0.3219452202320099.\u001b[0m\n",
            "\u001b[32m[I 2022-09-08 12:27:49,359]\u001b[0m Trial 17 finished with value: 1.013181209564209 and parameters: {'hidden': 260, 'hid_layers': 8}. Best is trial 6 with value: 0.3219452202320099.\u001b[0m\n",
            "\u001b[32m[I 2022-09-08 12:30:47,522]\u001b[0m Trial 18 finished with value: 1.0319358110427856 and parameters: {'hidden': 100, 'hid_layers': 12}. Best is trial 6 with value: 0.3219452202320099.\u001b[0m\n",
            "\u001b[32m[I 2022-09-08 12:32:34,804]\u001b[0m Trial 19 finished with value: 0.39337313175201416 and parameters: {'hidden': 400, 'hid_layers': 2}. Best is trial 6 with value: 0.3219452202320099.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained('/content/tokenizer')\n",
        "config = BertConfig(\n",
        "    vocab_size=14,  # we align this to the tokenizer vocab_size\n",
        "    max_position_embeddings=83,\n",
        "    hidden_size=study.best_params['hidden'],\n",
        "    num_attention_heads=10,\n",
        "    num_hidden_layers=study.best_params['hid_layers'],\n",
        "    type_vocab_size=1    \n",
        "    )\n",
        "model = BertForMaskedLM(config)"
      ],
      "metadata": {
        "id": "XjtpfEk2ybnd"
      },
      "id": "XjtpfEk2ybnd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# activate training model\n",
        "model.train()\n",
        "# initialize optimizer\n",
        "optim = AdamW(model.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "id": "VtJbvEgPioqO"
      },
      "id": "VtJbvEgPioqO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training using optimal parameters\n",
        "\n",
        "inputs = tokenizer.batch_encode_plus(x1, return_tensors=\"pt\",is_split_into_words=True)\n",
        "labels = tokenizer.batch_encode_plus(y1, return_tensors=\"pt\",is_split_into_words=True)\n",
        "inputs['labels']=labels['input_ids']\n",
        "dataset = Dataset.from_dict(inputs)\n",
        "dataset.set_format(\"torch\")\n",
        "train_dataloader = DataLoader(dataset, shuffle=True, batch_size=32)"
      ],
      "metadata": {
        "id": "CaxBfnFNs9Ay"
      },
      "id": "CaxBfnFNs9Ay",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5\n",
        "num_training_steps = num_epochs * len(train_dataloader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optim,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps,\n",
        ")\n",
        "# print(num_training_steps)"
      ],
      "metadata": {
        "id": "VclCij0VOCl8"
      },
      "id": "VclCij0VOCl8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in train_dataloader:\n",
        "    break\n",
        "{k: v.shape for k, v in batch.items()}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9wn66j2hfmX",
        "outputId": "9c363e3c-5d4f-4566-ad29-ea9869907323"
      },
      "id": "F9wn66j2hfmX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': torch.Size([32, 83]),\n",
              " 'token_type_ids': torch.Size([32, 83]),\n",
              " 'attention_mask': torch.Size([32, 83]),\n",
              " 'labels': torch.Size([32, 83])}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Torch', torch.__version__, 'CUDA', torch.version.cuda)\n",
        "print('Device:', torch.device('cuda:0'))\n",
        "device = torch.device('cuda:0')\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETWdZQ1jqj1u",
        "outputId": "25abe477-5d80-415a-b27c-0e21c8903863"
      },
      "id": "ETWdZQ1jqj1u",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch 1.12.1+cu113 CUDA 11.3\n",
            "Device: cuda:0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForMaskedLM(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(14, 400, padding_idx=0)\n",
              "      (position_embeddings): Embedding(83, 400)\n",
              "      (token_type_embeddings): Embedding(1, 400)\n",
              "      (LayerNorm): LayerNorm((400,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=400, out_features=400, bias=True)\n",
              "              (key): Linear(in_features=400, out_features=400, bias=True)\n",
              "              (value): Linear(in_features=400, out_features=400, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=400, out_features=400, bias=True)\n",
              "              (LayerNorm): LayerNorm((400,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=400, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=400, bias=True)\n",
              "            (LayerNorm): LayerNorm((400,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=400, out_features=400, bias=True)\n",
              "              (key): Linear(in_features=400, out_features=400, bias=True)\n",
              "              (value): Linear(in_features=400, out_features=400, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=400, out_features=400, bias=True)\n",
              "              (LayerNorm): LayerNorm((400,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=400, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=400, bias=True)\n",
              "            (LayerNorm): LayerNorm((400,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=400, out_features=400, bias=True)\n",
              "              (key): Linear(in_features=400, out_features=400, bias=True)\n",
              "              (value): Linear(in_features=400, out_features=400, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=400, out_features=400, bias=True)\n",
              "              (LayerNorm): LayerNorm((400,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=400, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=400, bias=True)\n",
              "            (LayerNorm): LayerNorm((400,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=400, out_features=400, bias=True)\n",
              "              (key): Linear(in_features=400, out_features=400, bias=True)\n",
              "              (value): Linear(in_features=400, out_features=400, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=400, out_features=400, bias=True)\n",
              "              (LayerNorm): LayerNorm((400,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=400, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=400, bias=True)\n",
              "            (LayerNorm): LayerNorm((400,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=400, out_features=400, bias=True)\n",
              "              (key): Linear(in_features=400, out_features=400, bias=True)\n",
              "              (value): Linear(in_features=400, out_features=400, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=400, out_features=400, bias=True)\n",
              "              (LayerNorm): LayerNorm((400,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=400, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=400, bias=True)\n",
              "            (LayerNorm): LayerNorm((400,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=400, out_features=400, bias=True)\n",
              "              (key): Linear(in_features=400, out_features=400, bias=True)\n",
              "              (value): Linear(in_features=400, out_features=400, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=400, out_features=400, bias=True)\n",
              "              (LayerNorm): LayerNorm((400,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=400, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=400, bias=True)\n",
              "            (LayerNorm): LayerNorm((400,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (cls): BertOnlyMLMHead(\n",
              "    (predictions): BertLMPredictionHead(\n",
              "      (transform): BertPredictionHeadTransform(\n",
              "        (dense): Linear(in_features=400, out_features=400, bias=True)\n",
              "        (transform_act_fn): GELUActivation()\n",
              "        (LayerNorm): LayerNorm((400,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "      (decoder): Linear(in_features=400, out_features=14, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "\n",
        "i=0\n",
        "model.train()\n",
        "best_loss = 2.0\n",
        "for epoch in range(num_epochs):\n",
        "  print(\"Epoch {}: \".format(epoch))\n",
        "  for batch in train_dataloader:\n",
        "      batch = {k: v.to(device) for k, v in batch.items()}\n",
        "      outputs = model(**batch)\n",
        "      loss = outputs.loss\n",
        "      if(loss.item() < best_loss):\n",
        "        model.save_pretrained('./sudoku-bert')\n",
        "        best_loss = loss.item()\n",
        "      loss.backward()\n",
        "      \n",
        "      i=i+1\n",
        "      if(i % 400 == 0):\n",
        "        print(\"Loss: {}  \".format(loss))\n",
        "      optim.step()\n",
        "      lr_scheduler.step()\n",
        "      optim.zero_grad()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdh0g5AhiclB",
        "outputId": "93734d13-5419-47cc-db7a-fb619454802c"
      },
      "id": "gdh0g5AhiclB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: \n",
            "Loss: 1.120271921157837  \n",
            "Loss: 1.066070795059204  \n",
            "Loss: 1.0977455377578735  \n",
            "Loss: 0.4316365420818329  \n",
            "Loss: 0.32883527874946594  \n",
            "Loss: 0.27505841851234436  \n",
            "Loss: 0.33567774295806885  \n",
            "Loss: 0.3259488642215729  \n",
            "Loss: 0.2550203204154968  \n",
            "Loss: 0.2629614770412445  \n",
            "Loss: 0.2479988932609558  \n",
            "Loss: 0.2497366964817047  \n",
            "Loss: 0.26964271068573  \n",
            "Loss: 0.2511368989944458  \n",
            "Loss: 0.23546437919139862  \n",
            "Loss: 0.21125775575637817  \n",
            "Loss: 0.26414594054222107  \n",
            "Loss: 0.21616625785827637  \n",
            "Epoch 1: \n",
            "Loss: 0.25771161913871765  \n",
            "Loss: 0.1707964688539505  \n",
            "Loss: 0.288177490234375  \n",
            "Loss: 0.26257002353668213  \n",
            "Loss: 0.20742423832416534  \n",
            "Loss: 0.20823460817337036  \n",
            "Loss: 0.2602976858615875  \n",
            "Loss: 0.1903720647096634  \n",
            "Loss: 0.16187891364097595  \n",
            "Loss: 0.14471352100372314  \n",
            "Loss: 0.17658931016921997  \n",
            "Loss: 0.20096434652805328  \n",
            "Loss: 0.17462414503097534  \n",
            "Loss: 0.1692923903465271  \n",
            "Loss: 0.14717702567577362  \n",
            "Loss: 0.1549849957227707  \n",
            "Loss: 0.18263743817806244  \n",
            "Loss: 0.13180172443389893  \n",
            "Loss: 0.10836561024188995  \n",
            "Epoch 2: \n",
            "Loss: 0.1805853396654129  \n",
            "Loss: 0.1375460922718048  \n",
            "Loss: 0.1801104098558426  \n",
            "Loss: 0.14979064464569092  \n",
            "Loss: 0.19143132865428925  \n",
            "Loss: 0.1635284125804901  \n",
            "Loss: 0.15984417498111725  \n",
            "Loss: 0.1877378225326538  \n",
            "Loss: 0.09930691123008728  \n",
            "Loss: 0.15011540055274963  \n",
            "Loss: 0.11702422797679901  \n",
            "Loss: 0.14649204909801483  \n",
            "Loss: 0.1407604217529297  \n",
            "Loss: 0.15731191635131836  \n",
            "Loss: 0.14973227679729462  \n",
            "Loss: 0.16302122175693512  \n",
            "Loss: 0.1970006376504898  \n",
            "Loss: 0.12209410965442657  \n",
            "Loss: 0.15122158825397491  \n",
            "Epoch 3: \n",
            "Loss: 0.12593068182468414  \n",
            "Loss: 0.15315017104148865  \n",
            "Loss: 0.14811374247074127  \n",
            "Loss: 0.13802367448806763  \n",
            "Loss: 0.1482982039451599  \n",
            "Loss: 0.10881072282791138  \n",
            "Loss: 0.12787935137748718  \n",
            "Loss: 0.09269727766513824  \n",
            "Loss: 0.09523095190525055  \n",
            "Loss: 0.09195958822965622  \n",
            "Loss: 0.0927308201789856  \n",
            "Loss: 0.17130067944526672  \n",
            "Loss: 0.13084812462329865  \n",
            "Loss: 0.11669574677944183  \n",
            "Loss: 0.07377311587333679  \n",
            "Loss: 0.09386593848466873  \n",
            "Loss: 0.1331549435853958  \n",
            "Loss: 0.12407364696264267  \n",
            "Loss: 0.09966276586055756  \n",
            "Epoch 4: \n",
            "Loss: 0.08634994924068451  \n",
            "Loss: 0.08713424205780029  \n",
            "Loss: 0.11918798834085464  \n",
            "Loss: 0.13472875952720642  \n",
            "Loss: 0.11875120550394058  \n",
            "Loss: 0.12595340609550476  \n",
            "Loss: 0.10716281086206436  \n",
            "Loss: 0.0903388187289238  \n",
            "Loss: 0.06877084076404572  \n",
            "Loss: 0.08592531830072403  \n",
            "Loss: 0.12635836005210876  \n",
            "Loss: 0.0881987139582634  \n",
            "Loss: 0.0946778729557991  \n",
            "Loss: 0.08118750900030136  \n",
            "Loss: 0.06460453569889069  \n",
            "Loss: 0.11010250449180603  \n",
            "Loss: 0.11065516620874405  \n",
            "Loss: 0.07598192989826202  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(best_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekvEl9WCHWbg",
        "outputId": "f6f749c3-4f9f-4da1-8830-0be84eb3aa79"
      },
      "id": "ekvEl9WCHWbg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.029785988852381706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/drive/MyDrive/sudoku-bert1.zip /content/sudoku-bert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7F31WgXdA1J",
        "outputId": "e4dd9d9f-4d34-40c1-dade-6c0af939a177"
      },
      "id": "R7F31WgXdA1J",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/sudoku-bert/ (stored 0%)\n",
            "  adding: content/sudoku-bert/pytorch_model.bin (deflated 7%)\n",
            "  adding: content/sudoku-bert/config.json (deflated 47%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "puzz = test.iloc[:, 0].values\n",
        "soln = test.iloc[:, 1].values\n",
        "x2 = list(map(sudoku_to_tokens, puzz))\n",
        "y2 = list(map(sudoku_to_tokens, soln))\n",
        "\n",
        "test_inputs = tokenizer.batch_encode_plus(x2, return_tensors=\"pt\",is_split_into_words=True)\n",
        "test_labels = tokenizer.batch_encode_plus(y2, return_tensors=\"pt\",is_split_into_words=True)\n",
        "test_inputs['labels'] = test_labels['input_ids']\n",
        "test_dataset = Dataset.from_dict(test_inputs)\n",
        "test_dataset.set_format(\"torch\")\n",
        "test_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=32)"
      ],
      "metadata": {
        "id": "nGXGHcddvCSH"
      },
      "id": "nGXGHcddvCSH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_model = BertForMaskedLM.from_pretrained('/content/drive/MyDrive/sudoku-bert')"
      ],
      "metadata": {
        "id": "W6GlVb9s-PbZ"
      },
      "id": "W6GlVb9s-PbZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_inputs = tokenizer.encode_plus(x2[3300], return_tensors=\"pt\",is_split_into_words=True)\n",
        "new_labels = tokenizer.encode_plus(y2[3300], return_tensors=\"pt\",is_split_into_words=True)"
      ],
      "metadata": {
        "id": "lpXZGMa4dOvL"
      },
      "id": "lpXZGMa4dOvL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    output = test_model(**new_inputs, labels=new_labels['input_ids'])\n",
        "    logits = output.logits\n",
        "\n",
        "# retrieve index of [MASK]\n",
        "mask_token_index = (new_inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
        "\n",
        "predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
        "print(output[0])\n",
        "print(predicted_token_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbraatp-dVf_",
        "outputId": "e0b63471-0297-46eb-b496-55997f709963"
      },
      "id": "vbraatp-dVf_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0017)\n",
            "tensor([ 8,  6, 11, 13, 12,  9, 13,  6,  5,  9,  8, 12,  8, 13, 11,  6,  6, 10,\n",
            "         7, 11, 12, 13, 10,  7,  5,  7,  9,  6, 10, 12,  8,  5,  6,  6,  5,  9,\n",
            "        10, 11,  8,  9, 12, 13, 10,  5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask_tokens = tokenizer.decode(predicted_token_id)\n",
        "print(mask_tokens)\n",
        "# why is there a [CLS] in there?\n",
        "mask_tokens = mask_tokens.replace(\"[CLS]\", \"one\") # just for testing\n",
        "mask_tokens = mask_tokens.replace(\"[SEP]\", \"one\") # just for testing\n",
        "mask_str = tokens_to_sudoku(mask_tokens.split(\" \"))\n",
        "print(mask_str)\n",
        "\n",
        "solution = fill_masks(tokens_to_sudoku(x2[3300]), mask_str)\n",
        "display_sudoku(solution)\n",
        "print(\"Correct?:\", check_correctness(solution))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cQhZbPidgpo",
        "outputId": "c69a38fc-b3d7-4d32-96fd-6d0c8b68d16f"
      },
      "id": "5cQhZbPidgpo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "four two seven nine eight five nine two one five four eight four nine seven two two six three seven eight nine six three one three five two six eight four one two two one five six seven four five eight nine six one\n",
            "42798592154849722637896313526841221567458961\n",
            "---------------------\n",
            "3 4 6 | 2 7 9 | 1 8 5 \n",
            "7 9 2 | 1 5 8 | 6 4 3 \n",
            "5 1 8 | 3 4 6 | 9 7 2 \n",
            "---------------------\n",
            "2 6 3 | 7 1 5 | 4 9 8 \n",
            "8 5 9 | 6 2 4 | 3 1 7 \n",
            "1 7 4 | 9 8 3 | 5 2 6 \n",
            "---------------------\n",
            "6 8 7 | 4 3 1 | 2 5 9 \n",
            "9 2 1 | 5 6 7 | 8 3 4 \n",
            "4 3 5 | 8 9 2 | 7 6 1 \n",
            "---------------------\n",
            "Correct?: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "given = x2[3300]\n",
        "input = tokens_to_sudoku(given)\n",
        "token_sudoku = y2[3300]\n",
        "answer = tokens_to_sudoku(token_sudoku)\n",
        "print(answer)\n",
        "display_sudoku(input)\n",
        "display_sudoku(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LO_fZRn0dl-x",
        "outputId": "cfba6353-2a13-42a8-b8b4-1964a0bd25ff"
      },
      "id": "LO_fZRn0dl-x",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "346279185792158643518346972263715498859624317174983526687431259921567834435892761\n",
            "---------------------\n",
            "3   6 |       | 1     \n",
            "7     |     8 | 6   3 \n",
            "5 1   | 3   6 |       \n",
            "---------------------\n",
            "      |   1 5 | 4 9 8 \n",
            "  5   |   2 4 |   1 7 \n",
            "  7 4 | 9 8   |       \n",
            "---------------------\n",
            "6   7 |   3   |   5 9 \n",
            "9     |       | 8 3 4 \n",
            "  3   |     2 | 7     \n",
            "---------------------\n",
            "---------------------\n",
            "3 4 6 | 2 7 9 | 1 8 5 \n",
            "7 9 2 | 1 5 8 | 6 4 3 \n",
            "5 1 8 | 3 4 6 | 9 7 2 \n",
            "---------------------\n",
            "2 6 3 | 7 1 5 | 4 9 8 \n",
            "8 5 9 | 6 2 4 | 3 1 7 \n",
            "1 7 4 | 9 8 3 | 5 2 6 \n",
            "---------------------\n",
            "6 8 7 | 4 3 1 | 2 5 9 \n",
            "9 2 1 | 5 6 7 | 8 3 4 \n",
            "4 3 5 | 8 9 2 | 7 6 1 \n",
            "---------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "## Validate model\n",
        "sum = 0\n",
        "elements = 0\n",
        "for i, (x,y) in enumerate(zip(x2, y2)):\n",
        "  new_inputs = tokenizer.encode_plus(x, return_tensors=\"pt\",is_split_into_words=True)\n",
        "  new_labels = tokenizer.encode_plus(y, return_tensors=\"pt\",is_split_into_words=True)\n",
        "  with torch.no_grad():\n",
        "      output = test_model(**new_inputs, labels=new_labels['input_ids'])\n",
        "      logits = output.logits\n",
        "\n",
        "  # retrieve index of [MASK]\n",
        "  mask_token_index = (new_inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
        "  predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
        "  correct_token_id = new_labels[\"input_ids\"][0, mask_token_index]\n",
        "  preds = tokenizer.decode(predicted_token_id)\n",
        "  num_matching_masks = np.count_nonzero(correct_token_id==predicted_token_id)\n",
        "  percent_correct = num_matching_masks / len(predicted_token_id)\n",
        "  sum += percent_correct\n",
        "  elements += 1\n",
        "\n",
        "  if i % 100 == 0:\n",
        "    print(f\"{i:04}: {num_matching_masks} / {len(predicted_token_id)} = {percent_correct:03f} | average: {sum / elements:03f}\")"
      ],
      "metadata": {
        "id": "Wm8f2NOswlJF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d3ba447c-1c44-4b0d-9b8d-a002abd21470"
      },
      "id": "Wm8f2NOswlJF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0000: 43 / 44 = 0.977273 | average: 0.977273\n",
            "0100: 44 / 45 = 0.977778 | average: 0.944623\n",
            "0200: 41 / 42 = 0.976190 | average: 0.943196\n",
            "0300: 40 / 43 = 0.930233 | average: 0.940204\n",
            "0400: 37 / 37 = 1.000000 | average: 0.935534\n",
            "0500: 39 / 39 = 1.000000 | average: 0.936724\n",
            "0600: 36 / 49 = 0.734694 | average: 0.938336\n",
            "0700: 40 / 47 = 0.851064 | average: 0.937918\n",
            "0800: 40 / 40 = 1.000000 | average: 0.938179\n",
            "0900: 44 / 44 = 1.000000 | average: 0.938619\n",
            "1000: 32 / 32 = 1.000000 | average: 0.937994\n",
            "1100: 45 / 45 = 1.000000 | average: 0.937159\n",
            "1200: 39 / 39 = 1.000000 | average: 0.937630\n",
            "1300: 37 / 39 = 0.948718 | average: 0.937413\n",
            "1400: 47 / 47 = 1.000000 | average: 0.936751\n",
            "1500: 39 / 39 = 1.000000 | average: 0.935934\n",
            "1600: 26 / 26 = 1.000000 | average: 0.936240\n",
            "1700: 37 / 37 = 1.000000 | average: 0.936696\n",
            "1800: 43 / 43 = 1.000000 | average: 0.937327\n",
            "1900: 43 / 43 = 1.000000 | average: 0.937654\n",
            "2000: 30 / 30 = 1.000000 | average: 0.937600\n",
            "2100: 45 / 47 = 0.957447 | average: 0.937985\n",
            "2200: 41 / 52 = 0.788462 | average: 0.937761\n",
            "2300: 39 / 50 = 0.780000 | average: 0.938709\n",
            "2400: 44 / 44 = 1.000000 | average: 0.938720\n",
            "2500: 31 / 31 = 1.000000 | average: 0.939009\n",
            "2600: 40 / 40 = 1.000000 | average: 0.938820\n",
            "2700: 41 / 41 = 1.000000 | average: 0.938819\n",
            "2800: 34 / 36 = 0.944444 | average: 0.938974\n",
            "2900: 33 / 33 = 1.000000 | average: 0.939387\n",
            "3000: 32 / 37 = 0.864865 | average: 0.939359\n",
            "3100: 41 / 47 = 0.872340 | average: 0.939063\n",
            "3200: 40 / 40 = 1.000000 | average: 0.939234\n",
            "3300: 44 / 44 = 1.000000 | average: 0.938782\n",
            "3400: 42 / 48 = 0.875000 | average: 0.939042\n",
            "3500: 43 / 44 = 0.977273 | average: 0.938685\n",
            "3600: 44 / 44 = 1.000000 | average: 0.938752\n",
            "3700: 35 / 35 = 1.000000 | average: 0.938985\n",
            "3800: 43 / 43 = 1.000000 | average: 0.938677\n",
            "3900: 45 / 45 = 1.000000 | average: 0.938916\n",
            "4000: 30 / 30 = 1.000000 | average: 0.938899\n",
            "4100: 37 / 37 = 1.000000 | average: 0.938872\n",
            "4200: 41 / 46 = 0.891304 | average: 0.938713\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-c71be7bdcc60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mnew_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_split_into_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m       \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mnew_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m       \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1360\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1363\u001b[0m         )\n\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m         )\n\u001b[1;32m   1030\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    612\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m                 )\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m         )\n\u001b[1;32m    500\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m         )\n\u001b[1;32m    432\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0mmixed_query_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;31m# If this is instantiated as a cross-attention module, the keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AGJOFcW4JQqw"
      },
      "id": "AGJOFcW4JQqw",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}